require 'reid'

requester_options = {
	:min_request_interval => 1,
	:max_backoff_time => 60
	#...
}

reid = Reid.new(requester_options)

##############################
# 	Single page scrape
##############################

operations = [
	[
		Proc.new{|element, record| record[:title] = element.xpath('//title').text},
		'//head',
		:xpath
	],
	[
		Proc.new{|element, record| record[:paragraph] = element.css('p').text},
		'body',
		:css
	]
]

record = reid.scrape_page('http://example.iana.org/', operations)

p record[:title] #=> "Example Domain"


##############################
# 	Using crawl method
##############################

class Url_iter
	def initialize
		@urls = ['http://example.iana.org/',
				 'http://www.iana.org/domains/special']
		@current = 0
	end
	def next(doc)
# This method should return urls until all urls are
#   processed, then it should return nil.
# 
# Nil is passed in on the first call.
# The Nokogiri document generated by the previous request is passed in on 
#   subsequent requests (you can check this to determine if you've reached
#	the end of the given crawl)
		if @current == 2
			@current = 0
			return nil
		else
			@current += 1
			return @urls[@current - 1]
		end
	end
end

persist_method = lambda do |record|
	#this is where you handle checking and persisting your record
	p 'I should be storing ' + record[:title]
end

reid.crawl(Url_iter.new, operations, persist_method)

#=> "I should be storing Example Domain"
#=> "I should be storing Iana..."